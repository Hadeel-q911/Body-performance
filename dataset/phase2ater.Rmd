---
title: "Body performance"
output: html_notebook
---

### Goal :
The primary objective of this project is to emphasize the significance
of body performance in enhancing physical health and promoting exercise
awareness among individuals. This will be accomplished by analyzing and
extracting valuable insights from body performance datasets, monitoring
the progression of human age, and subsequently improving the overall
awareness of its importance.

---------------------

### 1. Problem:


---------------------

### 2. Data Mining task:
??

#### Classification gaol :
we employ a classification approach to categorize individuals into distinct levels of fitness based on various data attributes, including body measurements such as weight, body fat percentage, and health indicators, as well as activity levels. Furthermore, we aim to predict performance outcomes by leveraging this collected data.

#### Clustering gaol :
Our primary goal in utilizing clustering on our dataset is to identify inherent patterns or groupings within the data. By applying clustering algorithms, we aim to partition the dataset into distinct clusters based on similarities or shared characteristics among the data points. This enables us to gain insights into the underlying structure of the data, discover relationships between data points, and potentially uncover valuable knowledge or trends that may be hidden within the dataset.

---------------------

### 3. Data:

#### 3.1: Data Source :
It from Kaggle website , The link(<https://www.kaggle.com/datasets/kukuroo3/body-performance-data>) show
the source data.

### 3.2: General Information about Body performance dataset :

Our dataset is called "Body performance" , The dataset provide the grade
of performance and some data such as gender and age along with body
measurements and other performance tests , It has 13394 row and each of
them have 11 attributes.

Table 1 : Data dictionary showing description of all Attributes

|name|description|data type|possible value|
|-----------------|---------------------|-----------------|-----------------|
| Age                     | The person's age in years                                                                             | numerical | 21-64          |
| Gender                  | The person's gender                                                                                   | binary    | F,M            |
| Height_cm               | The person's Height in cm                                                                             | numerical | 125-194        |
| weight_kg               | The person's weight in Kg                                                                             | numerical | 26.3-138       |
| body fat\_%             | the amount of essential fat .                                                                         | numerical | 3%-78.4%       |
| diastolic               | measures pressure the blood vessels when the heart is at rest                                         | numerical | 0-156          |
| systolic                | measures pressure in the arteries when the heart beats in minutes                                     | numerical | 0-201          |
| gripForce               | fingers flexibility tests                                                                             | numerical | 0-70.5         |
| sit and bend forward_cm | measures flexibility in sitting and bending forward in centimeters.                                   | numerical | -25-213        |
| sit-ups counts_cm       | measures the strength and endurance of the abdominals and hip-flexor muscles in centimeter.           | numerical | 0-80           |
| broad jump_cm           | It is a method of measuring how far a person can jump from a standing position to a landing position. | numerical | 0-303          |
|class| body performance score| numerical | A,B,C,D

----------------

### 3.3: Importing the dataset:

```{r}
dataset<- read.csv("bodyPerformance.csv")
View(dataset)
str(dataset)
```


------------------------------------------------------------------------

### 3.4: performance dataset before preprocessing:


```{r}
print(dataset)
```


```{r}
#Number of rows
nrow(dataset)
#Number of column
ncol(dataset)
```

------------------------------------------------------------------------

### Understanding the dataset using graph representations:

### Encoding: 
In order to represent the nominal attributes (gender) and (class) in graphs, we had to encode them as they cannot be directly displayed in their original form.
```{r}  
 #encoding Class: 
dataset$class = factor(dataset$class,levels =
                        c("A","B", "C","D"), labels = c(1, 2, 3,4))
```


```{r}
 #encoding Gender:
dataset$gender = factor(dataset$gender,levels =
                         c("F","M"), labels = c(0,1))
```


------------------------------------------------------------------------

### check if the dataset is balanced or imbalanced (Class boxplot) :

After encoding the attribute (class) We employ a Boxplot graph on our class label (class) to ascertain whether our dataset is balanced or not.

```{r}
boxplot(dataset$class, 
        ylab= "The person’s age in years boxplot", 
        main= "Boxplot of age")
```

Since the median is positioned in the middle of the box plot, it indicates that our dataset is balanced, and there is no need to perform any sampling.


------------------------------------------------------------------------

#### 1- Scatter plot for Gender and class :

first we endcode the gender F to be 0 and M to 1, then we use the plot to determine the relationship between gender and class attributes , we notice that the female have higher performance then male.

```{r}
plot(dataset$gender, dataset$class, main = " the performance between meal amd female", 
     xlab = "gender", ylab = "class",
     xlim = c(0,1), ylim = c(0,4))
```


------------------------------------------------------------------------

#### 2- Scatter plot of height and weight:

Observation:

 This scatter plot helps us determine whether height and
weight are correlated to each other or not. The plot shows a strong
positive correlation between the two attributes, indicating a
proportional relationship.

```{r}
plot(dataset$height_cm, dataset$weight_kg, main = "Body Measurement", 
     xlab = "height", ylab = "weight",
     xlim = c(100,200), ylim = c(20,150))
```

------------------------------------------------------------------------

#### 3- Histogram of body fat :

Observation:

  he histogram represents the frequency distribution of body
fat values. We observe that the majority of values lie within the range
of 10-30. However, the histogram also reveals the presence of outliers
in the dataset .

```{r}
hist(dataset$body.fat_.)
```


------------------------------------------------------------------------

#### 4- Scatter plot for age and diastoic :
Observation :

  This scatter plot helps us determine the correlation between Age and
diastolic. The plot shows no correlation between the two attributes,
indicating no proportional relationship

```{r}
plot(dataset$age, dataset$diastolic, main = "Bold pressure of Males abd famales", 
     xlab = "age", ylab = "diastolic",
     xlim = c(20,64), ylim = c(0,200))
```


------------------------------------------------------------------------

### Statistical Measures:

```{r}
 summary(dataset$age)
```


```{r}
summary(dataset$height_cm)
```


```{r}
 summary(dataset$weight_kg)
```


```{r}
 summary(dataset$body.fat_.)
```


```{r}
 summary(dataset$diastolic)
```


```{r}
 summary(dataset$systolic)
```


```{r}
 summary(dataset$gripForce)
```


```{r}
 summary(dataset$sit.and.bend.forward_cm)
```


```{r}
summary(dataset$sit.ups.counts)
```


```{r}
 summary(dataset$broad.jump_cm)
```


------------------------------------------------------------------------

## Data Preprocessing:

### Check duplicated rows:
 We check if there are any duplicated rows to decrease dataset volume. Hence enhancing data quality which leads to better model performance and speed.

witch there's no duplicated rows as showing:

```{r}
sum(duplicated(dataset))
```

we found 1 row that’s duplicate that need to be deleted.


```{r}
#Remove duplicated rows 
dataset <- dataset[!duplicated (dataset), ]
```


```{r}
#To make sure that the deletion was successful 
sum(duplicated(dataset))
```

### Check nulls value:
Null values can cause issues when performing data analysis or building machine learning models, as they can lead to inaccurate results or errors, where we found then 0 null in our dataset. So, no need to delete any.

```{r}
#to find the total null values in the dataset
sum(is.na(dataset))
```

------------------------------------------------------------------------

## Detect Outlier and Remove them:
 We checked our dataset to see if there were any outlier values, and we found a couple. Statistical studies and modeling can be significantly impacted by the presence of outliers. Outliers can skew results and impair the validity and reliability of inferences made from the research since they are data points that differ dramatically from the majority of the data. Therefore, in order to prevent them from harming our outcomes, we must erase them before we begin our task.

An outlier detection approach was used on the dataset to solve this problem. The detection approach used makes use of the Outliers package's Outlier() function. This method revealed the outliers among the data points.

```{r}
#install outliers package:
 library(outliers)
```


------------------------------------------------------------------------

#### boxplot to detect outliers:

 From The person’s age in years boxplot and summary we notice that there are no outliers showing on the boxplot , Also, we observe that the mean is less than the median, indicating a down skew (negatively skewed) in the distribution. This suggests that the age distribution is stretched towards the lower values, with more individuals falling in the younger age range.

```{r}
# age boxplot :
boxplot(dataset$age , 
        ylab= "The person’s age in years boxplot", 
        main= "Boxplot of age")
```

------------------------------------------------------------------------


 The summary and box plot values for The person’s height in cm indicate a dataset ranging from 125.0 to 193.8. the median at 169.2, and the mean is 168.6. Comparing the mean and median, we observe that they are relatively close in value. Also, there is some outliers that we will handle it by deleting them in preprocessing steps.
 
```{r}
#Hight boxplot :
boxplot(dataset$height_cm , 
        ylab= "The person’s height in cm", 
        main= "Boxplot of height_cm")

```

------------------------------------------------------------------------


The summary and box plot values for The person’s weight in Kg indicate a dataset ranging from  26.30 to 138.10. The mean weight is 67.45. Comparing the mean and median, we observe that they are relatively close in value, and that due also the maximum outliers.

```{r}

#Weight boxplot;
boxplot(dataset$weight_kg , 
        ylab= "The person’s weight in Kg", 
        main= "Boxplot of weight")

```

------------------------------------------------------------------------

The boxplot of the person's body fat (the amount of essential fat .)  shows how the values in the dataset have been evenly distributed around the median value of 22.80, and, there some outliers that we will handle them.

```{r}
#body fat boxplot : 
boxplot(dataset$body.fat_.,
        ylab= "the person's body fat" ,
        main= "Boxplot of body fat_%")
```

------------------------------------------------------------------------

The boxplot of the person's diastolic (measures pressure the blood vessels when the heart is at rest) shows how the values in the dataset have been evenly distributed around the median value of 79.0, and, there some outliers that we will handle them

```{r}
#diastolic box plot :
boxplot(dataset$diastolic,
        ylab= "the person's diastolic ",
        main= "Boxplot of diastolic")
```

------------------------------------------------------------------------

The boxplot of the person's systolic (measures pressure in the arteries when the heart beats in minutes) shows how the values in the dataset have been evenly distributed around the median value of 130.0, and, there some outliers that we will handle them.

```{r}
#systolic box plot:
boxplot(dataset$systolic,
        ylab= "The person's systolic " ,
        main= "Boxplot of systolic")
```

------------------------------------------------------------------------

The boxplot of gripForce (fingers flexibility tests) shows how the values in the dataset have been evenly distributed around the median value of 37.90, and, there some outliers that we will handle them

```{r}
#gripForce box plot:
boxplot(dataset$gripForc,
        ylab= "the person's gripForce " ,
        main= "Boxplot of gripForc")

```

------------------------------------------------------------------------

 the test that's measures flexibility in sitting and bending forward in centimeters” boxplot illustrates a slight down skew (negatively skewed) in the distribution, where there are more extreme values on the lower end(outliers) that we will handle it in preprocessing step by deleting them.

```{r}
#sit and bend forward_cm box plot: 
boxplot(dataset$sit.and.bend.forward_cm,
        ylab= "sit and bend forward_cm" ,
        main= "Boxplot of sit and bend forward_cm")
```


------------------------------------------------------------------------

The boxplot of the "Test that measures the strength and endurance of the abdominals and hip-fle" shows how the values in the dataset have been evenly distributed around the median value of 41, and, there's no outliers showing on boxplot.

```{r}
#sit-ups counts_cm box plot:
boxplot(dataset$sit.ups.counts,
        ylab= "sit-ups" ,
        main= "Boxplot of sit-ups")
```


------------------------------------------------------------------------

The boxplot of the "method of measuring how far a person can jump from a standing position to a landing position" shows how the values in the dataset have been evenly distributed around the median value of 193, and, there some minimum outliers that we will handle them in preprocessing step by deleting them.

```{r}
#broad jump_cm box plot:
boxplot(dataset$broad.jump_cm,
        ylab= "broad jump_cm " ,
        main= "Boxplot of broad jump_cm ")
```



------------------------------------------------------------------------


### Detect outlirs:

We noticed that "Age" has an Outlier that it's not showing on the boxplot, where its an Outlier within the range. that's mean those values are negatively affects on the dataset even though it is within range. So, we delete them

```{r}
#Detect outlir for 'age': 
outAge = outlier(dataset$age, logical =TRUE)

#number of outliers:

sum(outAge)
Find_outlier = which(outAge ==TRUE, arr.ind = TRUE)
outAge
Find_outlier

#Delete age outliers:
dataset= dataset[-Find_outlier,]

```


------------------------------------------------------------------------


```{r}
#Detect outlir for 'height_cm': 
outH = outlier(dataset$height_cm, logical =TRUE)
#number of outliers:
sum(outH)
Find_outlier = which(outH ==TRUE, arr.ind = TRUE)
outH
Find_outlier

#Delete height_cm outliers:
dataset= dataset[-Find_outlier,]
```

------------------------------------------------------------------------


```{r}
#Detect outlir for 'weight_kg': 
outW= outlier(dataset$weight_kg, logical =TRUE)
#number of outliers:
sum(outW)
Find_outlier = which(outW ==TRUE, arr.ind = TRUE)
outW
Find_outlier

#Delete weight_kg outliers:
dataset= dataset[-Find_outlier,]
```


------------------------------------------------------------------------

```{r}
#Detect outlir for 'body.fat\_.': 

outefat = outlier(dataset$body.fat_., logical =TRUE)
#number of outliers:
sum(outefat)
Find_outlier = which(outefat ==TRUE, arr.ind = TRUE)
outefat
Find_outlier

#Delete body.fat outliers:
dataset= dataset[-Find_outlier,]
```


------------------------------------------------------------------------


```{r}
#Detect outlir for 'systolic': 
outesys = outlier(dataset$systolic, logical =TRUE)
#number of outliers:
sum(outesys)
Find_outlier = which(outesys ==TRUE, arr.ind = TRUE)
outesys
Find_outlier

#Delete systolic outliers:
dataset= dataset[-Find_outlier,]
```

------------------------------------------------------------------------


 As we saw in boxplot there was only one outlier. However, here we detected 3 values, where 2 are within the range but we will take it as an outliers sinces those values may have a negatively affects on the dataset specifically on "class" attribute.
 
```{r}
#Detect outlir for 'gripForce': 
outegrip = outlier(dataset$gripForce, logical =TRUE)
#number of outliers:
sum(outegrip)
Find_outlier = which(outegrip ==TRUE, arr.ind = TRUE)
outegrip
Find_outlier

#Delete gripForce  outliers:
dataset= dataset[-Find_outlier,]
```


------------------------------------------------------------------------


```{r}
#Detect outlir for 'sit.and.bend.forward_cm': 
outesit = outlier(dataset$sit.and.bend.forward_cm, logical =TRUE)
#number of outliers:
sum(outesit)
Find_outlier = which(outesit ==TRUE, arr.ind = TRUE)
outesit
Find_outlier

#Delete sit.and.bend.forward_cm outliers:
dataset= dataset[-Find_outlier,]
```

------------------------------------------------------------------------


we notic some outliers within the range and we will deleting them as an outliers since it's effects other attributes negatively.

```{r}
#Detect outlir for 'sit.up.count': 
outesitup = outlier(dataset$sit.ups.counts, logical =TRUE)
#number of outliers:
sum(outesitup)
Find_outlier = which(outesitup ==TRUE, arr.ind = TRUE)
outesitup
Find_outlier

#Delete age outliers:
dataset= dataset[-Find_outlier,]
```

------------------------------------------------------------------------

```{r}
#Detect outlir for 'broad.jump_cm': 

Outjump = outlier(dataset$broad.jump_cm, logical =TRUE)
sum(Outjump)
Find_outlier = which(Outjump ==TRUE, arr.ind = TRUE)
Outjump
Find_outlier

dataset= dataset[-Find_outlier,]
```

------------------------------------------------------------------------

### Check rows and columns after deletion the outliers:

To make sure that the deletion was successful, we searched for the number of rows.

```{r}
#check after deleting
#Number of rows and column
nrow(dataset)
ncol(dataset)
```

We noticed a decrease in the number of rows, which means that deletion the outliers was successful.

------------------------------------------------------------------------

### Discretization :

We transformed the continuous vales of the age into intervals by dividing the values to be fall on one of three possible interval labels with equal width which is ( 21, 35],( 35, 49] and ( 49, 63] by discretisation the values well be meaningful and simpler to classify or perform other methods that can help us later in our model.

```{r}
#ه encoding للان بنحذه او بنسوي له
# Define bins for discretization
#dataset$age=cut(dataset$age, breaks = seq(21,64, by=14), right=FALSE)
```


------------------------------------------------------------------------

### Normalizetion:

 In order to ensure comparability across individuals and eliminate biases resulting from differences in measurement units or scales, we have decided to normalize the following attributes: height (in centimeters), weight (in kilograms), body fat percentage, diastolic, systolic, grip force, sit and bend forward distance (in centimeters), sit-ups count, and broad jump distance (in centimeters).
 
```{r}
View(dataset)
dataset [,3:4:5:6:7:8:9:11]=scale(dataset [,3:4:5:6:7:8:9:11])
```


---------------------------------

### Feature Selection :

##Chi-square for contingency data:

```{r}
#gender attribute
contingency_table <- table(dataset$gender, dataset$class)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```


```{r}
#age attribute
contingency_table <- table(dataset$age, dataset$class)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```


```{r}
#height_cm attribute
contingency_table <- table(dataset$height_cm, dataset$class)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```


```{r}
#weight_kg attribute
contingency_table <- table(dataset$weight_kg, dataset$class)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```


```{r}
#body.fat_.  attribute
contingency_table <- table(dataset$body.fat_., dataset$class)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```


```{r}
#diastolic  attribute
contingency_table <- table(dataset$diastolic, dataset$class)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```


```{r}
#systolic attribute
contingency_table <- table(dataset$systolic, dataset$class)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```


```{r}
#gripForce attribute
contingency_table <- table(dataset$gripForce, dataset$class)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```


```{r}
#sit.and.bend.forward_cm attribute
contingency_table <- table(dataset$sit.and.bend.forward_cm, dataset$class)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```


```{r}
#sit.ups.counts attribute
contingency_table <- table(dataset$sit.ups.counts, dataset$class)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```


```{r}
#broad.jump_cm attribute
contingency_table <- table(dataset$broad.jump_cm, dataset$class)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```



```{r}
# Load required libraries
library(corrplot)
library(dplyr)
library(caret)

#(نسوي انكودينق افضل او نلغي الديسكرايتيزيشن ويصير هذا ماله داعي لان اوردي نيومريك)
# Convert factor attribute to numeric
#dataset$age <- as.numeric(dataset$age)
# Verify the conversion
#str(dataset$age)

# Convert factor attribute to numeric
dataset$gender <- as.numeric(as.character(dataset$gender))

# Verify the conversion
str(dataset$gender)
# Calculate the correlation matrix for numeric attributes
correlation_matrix <- cor(dataset[, 1:10])

# Display the correlation matrix
corrplot(correlation_matrix, method = "color")

# Find highly correlated attributes
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.7)

# List the names of highly correlated attributes
highly_correlated_names <- names(dataset[, 1:10][, highly_correlated])

# Remove the highly correlated attributes from the dataset
reduced_dataset <-dataset %>%
  select(-one_of(highly_correlated_names))

# Verify the reduced dataset
head(reduced_dataset)


ncol(dataset)
nrow(dataset)

```

Based on the dataset and the graphs we did earlier, there are several justifications for not performing feature selection:

#### 1. Dimensionality: 
The number of attributes (12) is not excessively high, and it is within a manageable range for modeling and analysis.

#### 2.Correlation Assessment: 
just because some graphs indicate a lack of correlation between certain attributes, it doesn't imply that those attributes hold no value or provide no assistance in enhancing individual body performance. It is important to recognize the interplay between these attributes and their impact on body performance. Consequently, removing any of these attributes may result in loss of valuable information.

#### 3. Interpretability: 
By considering all the attributes, you can gain a deeper understanding of the relationships between different variables, and allows for a more comprehensive analysis providing more meaningful insights.

While feature selection can be beneficial in certain situations, in the case of analyzing body performance for individuals, it is important to consider all attributes without performing feature selection.

----------------------------------------------

1. Problem: 

-------------------
2.Data Mining task:

### 5- Data Mining Task:
****************description اتوقع مكانها غلظ ****************

#### 5.1 Classification: 

----------------------------------------------

#### 5.2 Clustering: حطيت نفس اكواد اللاب تحتاج تعديل كثير

--------------------- 


9.References
-----------------------------------------------------------------------------------------


  1. prepreocessing 
Data types should be transformed into numeric types before clustering.

1st we check data types:
```{r}
str(dataset)
```


----------------------------------------------

then converts the factors attribute to numeric type

```{r}
#!!!!!!!!!!!!!!!!!!!! don't run it نحط هنا او عند الفيتشر سلكشين انكودينق له
# Convert factor attribute to numeric
#dataset$age <- as.numeric(dataset$age)

# Verify the conversion
#str(dataset$age)
```


```{r}
# Convert factor attribute to numeric
dataset$gender <- as.numeric(as.character(dataset$gender))

# Verify the conversion
str(dataset$gender)
```


```{r}
# Convert factor attribute to numeric
dataset$class <- as.numeric(dataset$class)

# Verify the conversion
str(dataset$class)
```
 
#encoding Gender:
#dataset$gender = factor(dataset$gender,levels =
                         c("( 21, 35]","( 35, 49]", "( 49, 63]" ), labels = c(0,1,2))

#SOS
#OR بدال ال2 تشنك الي فوق نستخدم هذا :
#dataset <- scale(dataset) ??


```{r بينحذف}
str(dataset)
```


----------------------------------------------

#__________
#هذا الكلام لنا بعدين بنحذفه

#_____________________________________________
  loop for K number:
1- k-means
2-k-mediods
3-Hierarchical (silhouette plot of pam مفروض يطلع لنا )
4- Validation

note: #الافرج (الفالديشين عموما) مقروض اول خطوه بالبروجكت عشان نعرف كم كلستر نستخدم, لكن كيف ما فهمت هل الخطوات الي انا كاتبتها صح؟
#_____________________________________________



#From Project Description:   
-Apply K-means clustering to your dataset using at least three different sizes of K.
You have to justify your choice for size K (we tried k=4, k= ,k=)

-then compare and discuss the results using different evaluation methods and metrics
(Silhouette coefficient(we use it) , 
total within-cluster sum of square(did we?), 
BCubed precision and recall (did we?). 

-Use visual
representation and try your best to interpret the results and to understand the algorithms performance.

-you have to try at least three different sizes of K and report the 
Silhouette coefficient,
the total within-cluster sum of squares,
BCubed precision and recall. 
#__________


#determine and visualize optimal number of clusters: 

```{r}
library(factoextra)
fviz_nbclust(data, kmeans, method="wss")
fviz_nbclust(data, kmeans, method="silhouette")
fviz_nbclust(data, kmeans, method="gap_start")
```


#__________________________________________k=2________________________________________________________________

#---------------------------k-means clustering 2---------------------------------------

#kmeansruns() : It calls  kmeans() to perform  k-means clustering
#It initializes the k-means algorithm several times with random points from the data set as means.
#It estimates the number of clusters by Calinski Harabasz index or average silhouette width يقلل عدد الكلستر 

#### K= 2 Clusters

```{r}
# run k-means clustering to find 2 clusters
#set a seed for random number generation  to make the results reproducible
set.seed(8953)
kmeans.result <- kmeans(dataset, 2)
# print the clusterng result
kmeans.result
```

#### visualize 2 clusters

```{r}
# visualize clustering (2 clusters)
#install.packages("factoextra")
library(factoextra)
fviz_cluster(kmeans.result, data = dataset)
```


#fviz_cluster(kmeans.result, data = dataset)


```{r}
plot(dataset[, c("age","class")], col = (kmeans.result$cluster) )
# this command add Points(center) to a Plot 
points(kmeans.result$centers[, c("age","class")],  col = 1:4, pch = 8, cex=10) 
```


```{r}
#performance k-means clustering
kmeans.result <- kmeans(dataset, 2)

# creat the initial plot
plot(dataset[, c("weight_kg","class")], col = (kmeans.result$cluster) )

# this command add Points(center) to a Plot (عشان حطينا 10)
points(kmeans.result$centers[, c("weight_kg","class")],  col = 1:4, pch = 8, cex=2)  
```


```{r}
#performance k-means clustering
kmeans.result <- kmeans(dataset, 2)

# creat the initial plot
plot(dataset[, c("body.fat_.","class")], col = (kmeans.result$cluster) )

# this command add Points(center) to a Plot (عشان حطينا 10)
points(kmeans.result$centers[, c("body.fat_.","class")],  col = 1:4, pch = 8, cex=2)  
```


#--------------------k-means clustering 2 ???????---------------------------
  
#kmeansruns() : It calls  kmeans() to perform  k-means clustering
#It initializes the k-means algorithm several times with random points from the data set as means.
#It estimates the number of clusters by Calinski Harabasz index or average silhouette width يقلل عدد الكلستر 

```{r}
#install.packages("fpc")
library(fpc)
kmeansruns.result <- kmeansruns(dataset)  
kmeansruns.result #4 mean cuz we have 4dimension
fviz_cluster(kmeansruns.result, data = dataset)
```


```{r مب لازم يمكن}
#install.packages("cluster")
library(cluster)

# group into 4 clusters
pam.result <- pam(dataset, 2)

plot(pam.result)

#if some got -1 -> not on correct cluster
#if 0-> overlap 
```


```{r هل مكانه غلط؟}
#average silhouette for each clusters (2 clusters)
library(cluster)
avg_sil <- silhouette(kmeans.result$cluster,dist(dataset)) #a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
fviz_silhouette(avg_sil)#k-means clustering with estimating k and initializations
```


#----------------------------Hierarchical Clustering---------------------------------

##----Hierarchical Clustering of the USArrest Data-----##


```{r}
set.seed(2835)

# draw a sample of 40 records from the body performance data, so that the clustering plot will not be over crowded
idx <- sample(1:dim(dataset)[1], 40)
dataset <- dataset[idx, ]
```


```{r}
## hiercrchical clustering
library(factoextra) 
hc.cut <- hcut(dataset, k = 2, hc_method = "complete") # Computes Hierarchical Clustering and Cut the Tree
```


```{r}
#ليه حط الطول؟
# Visualize dendrogram
fviz_dend(hc.cut,rect = TRUE)  #logical value specifying whether to add a rectangle around groups.
```


```{r}
# Visualize cluster
fviz_cluster(hc.cut, ellipse.type = "convex") # Character specifying frame type. Possible values are 'convex', 'confidence' etc
```


#average silhouette for each clusters 
  - choosing k using average silhouette
Silhouette analysis

Silhouette analysis allows you to calculate how similar each observations is with the cluster it is assigned relative to other clusters. This metric (silhouette width) ranges from -1 to 1 for each observation in your data and can be interpreted as follows:

    Values close to 1 suggest that the observation is well matched to the assigned cluster
    Values close to 0 suggest that the observation is borderline matched between two clusters
    Values close to -1 suggest that the observations may be assigned to the wrong cluster

In this exercise you will leverage the pam() and the silhouette() functions from the cluster library to perform silhouette analysis to compare the results of models with a k of 2 and a k of 3. You’ll continue working with the lineup dataset.

    Pay close attention to the silhouette plot, does each observation clearly belong to its assigned cluster for k 
  
تفسيرنا


```{r}
#average silhouette for each clusters 
library(cluster)

#a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
avg_sil <- silhouette(kmeans.result$cluster,dist(dataset)) 

#k-means clustering with estimating k and initializations
fviz_silhouette(avg_sil)
```
  
  
```{r وين نحطها؟}
# 1-  define function to compute average silhouette for k clusters using silhouette()
silhouette_score <- function(k){ 
km <- kmeans(dataset, centers = k,nstart=25) # if centers is a number, how many random sets should be chosen?
ss <- silhouette(km$cluster, dist(dataset))
sil<- mean(ss[, 3])
return(sil)
}
```


#__________________________________________k=3________________________________________________________________

#---------------------------k-means clustering 3---------------------------------------

#kmeansruns() : It calls  kmeans() to perform  k-means clustering
#It initializes the k-means algorithm several times with random points from the data set as means.
#It estimates the number of clusters by Calinski Harabasz index or average silhouette width يقلل عدد الكلستر 

#### K= 3 Clusters

```{r}
# run k-means clustering to find 3 clusters
#set a seed for random number generation  to make the results reproducible
set.seed(8953)
kmeans.result <- kmeans(dataset, 3)
# print the clusterng result
kmeans.result
```

#### visualize 2 clusters

```{r}
# visualize clustering (2 clusters)
#install.packages("factoextra")
library(factoextra)
fviz_cluster(kmeans.result, data = dataset)
```


#fviz_cluster(kmeans.result, data = dataset)


```{r}
plot(dataset[, c("age","class")], col = (kmeans.result$cluster) )
# this command add Points(center) to a Plot 
points(kmeans.result$centers[, c("age","class")],  col = 1:4, pch = 8, cex=10) 
```


```{r}
#performance k-means clustering
kmeans.result <- kmeans(dataset, 3)

# creat the initial plot
plot(dataset[, c("weight_kg","class")], col = (kmeans.result$cluster) )

# this command add Points(center) to a Plot (عشان حطينا 10)
points(kmeans.result$centers[, c("weight_kg","class")],  col = 1:4, pch = 8, cex=2)  
```


```{r}
#performance k-means clustering
kmeans.result <- kmeans(dataset, 3)

# creat the initial plot
plot(dataset[, c("body.fat_.","class")], col = (kmeans.result$cluster) )

# this command add Points(center) to a Plot (عشان حطينا 10)
points(kmeans.result$centers[, c("body.fat_.","class")],  col = 1:4, pch = 8, cex=2)  
```


#--------------------k-means clustering 2 ???????---------------------------
  
#kmeansruns() : It calls  kmeans() to perform  k-means clustering
#It initializes the k-means algorithm several times with random points from the data set as means.
#It estimates the number of clusters by Calinski Harabasz index or average silhouette width يقلل عدد الكلستر 

```{r}
#install.packages("fpc")
library(fpc)
kmeansruns.result <- kmeansruns(dataset)  
kmeansruns.result #4 mean cuz we have 4dimension
fviz_cluster(kmeansruns.result, data = dataset)
```


```{r}
#install.packages("cluster")
library(cluster)

# group into 4 clusters
pam.result <- pam(dataset, 3)

plot(pam.result)

#if some got -1 -> not on correct cluster
#if 0-> overlap 
```


```{r}
#average silhouette for each clusters (2 clusters)
library(cluster)
avg_sil <- silhouette(kmeans.result$cluster,dist(dataset)) #a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
fviz_silhouette(avg_sil)#k-means clustering with estimating k and initializations
```


#----------------------------Hierarchical Clustering---------------------------------

##----Hierarchical Clustering of the USArrest Data-----##


```{r}
set.seed(2835)

# draw a sample of 40 records from the body performance data, so that the clustering plot will not be over crowded
idx <- sample(1:dim(dataset)[1], 40)
dataset <- dataset[idx, ]
```


```{r}
## hiercrchical clustering
library(factoextra) 
hc.cut <- hcut(dataset, k = 3, hc_method = "complete") # Computes Hierarchical Clustering and Cut the Tree
```


```{r}
#ليه حط الطول؟
# Visualize dendrogram
fviz_dend(hc.cut,rect = TRUE)  #logical value specifying whether to add a rectangle around groups.
```


```{r}
# Visualize cluster
fviz_cluster(hc.cut, ellipse.type = "convex") # Character specifying frame type. Possible values are 'convex', 'confidence' etc
```


#average silhouette for each clusters 
  - choosing k using average silhouette
Silhouette analysis

Silhouette analysis allows you to calculate how similar each observations is with the cluster it is assigned relative to other clusters. This metric (silhouette width) ranges from -1 to 1 for each observation in your data and can be interpreted as follows:

    Values close to 1 suggest that the observation is well matched to the assigned cluster
    Values close to 0 suggest that the observation is borderline matched between two clusters
    Values close to -1 suggest that the observations may be assigned to the wrong cluster

In this exercise you will leverage the pam() and the silhouette() functions from the cluster library to perform silhouette analysis to compare the results of models with a k of 2 and a k of 3. You’ll continue working with the lineup dataset.

    Pay close attention to the silhouette plot, does each observation clearly belong to its assigned cluster for k 
  
تفسيرنا


```{r}
#average silhouette for each clusters 
library(cluster)

#a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
avg_sil <- silhouette(kmeans.result$cluster,dist(dataset)) 

#k-means clustering with estimating k and initializations
fviz_silhouette(avg_sil)
```
  
  
```{r}
# 1-  define function to compute average silhouette for k clusters using silhouette()
silhouette_score <- function(k){ 
km <- kmeans(dataset, centers = k,nstart=25) # if centers is a number, how many random sets should be chosen?
ss <- silhouette(km$cluster, dist(dataset))
sil<- mean(ss[, 3])
return(sil)
}
```


#__________________________________________k=4________________________________________________________________

#---------------------------k-means clustering 4---------------------------------------

#kmeansruns() : It calls  kmeans() to perform  k-means clustering
#It initializes the k-means algorithm several times with random points from the data set as means.
#It estimates the number of clusters by Calinski Harabasz index or average silhouette width يقلل عدد الكلستر 

#### K= 2 Clusters

```{r}
# run k-means clustering to find 4 clusters
#set a seed for random number generation  to make the results reproducible
set.seed(8953)
kmeans.result <- kmeans(dataset, 4)
# print the clusterng result
kmeans.result
```

#### visualize 4 clusters

```{r}
# visualize clustering (4 clusters)
#install.packages("factoextra")
library(factoextra)
fviz_cluster(kmeans.result, data = dataset)
```


#fviz_cluster(kmeans.result, data = dataset)


```{r}
plot(dataset[, c("age","class")], col = (kmeans.result$cluster) )
# this command add Points(center) to a Plot 
points(kmeans.result$centers[, c("age","class")],  col = 1:4, pch = 8, cex=10) 
```


```{r}
#performance k-means clustering
kmeans.result <- kmeans(dataset, 4)

# creat the initial plot
plot(dataset[, c("weight_kg","class")], col = (kmeans.result$cluster) )

# this command add Points(center) to a Plot (عشان حطينا 10)
points(kmeans.result$centers[, c("weight_kg","class")],  col = 1:4, pch = 8, cex=2)  
```


```{r}
#performance k-means clustering
kmeans.result <- kmeans(dataset, 4)

# creat the initial plot
plot(dataset[, c("body.fat_.","class")], col = (kmeans.result$cluster) )

# this command add Points(center) to a Plot (عشان حطينا 10)
points(kmeans.result$centers[, c("body.fat_.","class")],  col = 1:4, pch = 8, cex=2)  
```


#--------------------k-means clustering 2 ???????---------------------------
  
#kmeansruns() : It calls  kmeans() to perform  k-means clustering
#It initializes the k-means algorithm several times with random points from the data set as means.
#It estimates the number of clusters by Calinski Harabasz index or average silhouette width يقلل عدد الكلستر 

```{r}
#install.packages("fpc")
library(fpc)
kmeansruns.result <- kmeansruns(dataset)  
kmeansruns.result #4 mean cuz we have 4dimension
fviz_cluster(kmeansruns.result, data = dataset)
```


```{r}
#install.packages("cluster")
library(cluster)

# group into 4 clusters
pam.result <- pam(dataset, 4)

plot(pam.result)

#if some got -1 -> not on correct cluster
#if 0-> overlap 
```


```{r}
#average silhouette for each clusters (4 clusters)
library(cluster)
avg_sil <- silhouette(kmeans.result$cluster,dist(dataset)) #a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
fviz_silhouette(avg_sil)#k-means clustering with estimating k and initializations
```


#----------------------------Hierarchical Clustering---------------------------------

##----Hierarchical Clustering of the USArrest Data-----##


```{r}
set.seed(2835)

# draw a sample of 40 records from the body performance data, so that the clustering plot will not be over crowded
idx <- sample(1:dim(dataset)[1], 40)
dataset <- dataset[idx, ]
```


```{r}
## hiercrchical clustering
library(factoextra) 
hc.cut <- hcut(dataset, k = 4, hc_method = "complete") # Computes Hierarchical Clustering and Cut the Tree
```


```{r}
#ليه حط الطول؟
# Visualize dendrogram
fviz_dend(hc.cut,rect = TRUE)  #logical value specifying whether to add a rectangle around groups.
```


```{r}
# Visualize cluster
fviz_cluster(hc.cut, ellipse.type = "convex") # Character specifying frame type. Possible values are 'convex', 'confidence' etc
```


#average silhouette for each clusters 
  - choosing k using average silhouette
Silhouette analysis

Silhouette analysis allows you to calculate how similar each observations is with the cluster it is assigned relative to other clusters. This metric (silhouette width) ranges from -1 to 1 for each observation in your data and can be interpreted as follows:

    Values close to 1 suggest that the observation is well matched to the assigned cluster
    Values close to 0 suggest that the observation is borderline matched between two clusters
    Values close to -1 suggest that the observations may be assigned to the wrong cluster

In this exercise you will leverage the pam() and the silhouette() functions from the cluster library to perform silhouette analysis to compare the results of models with a k of 2 and a k of 3. You’ll continue working with the lineup dataset.

    Pay close attention to the silhouette plot, does each observation clearly belong to its assigned cluster for k 
  
تفسيرنا


```{r}
#average silhouette for each clusters 
library(cluster)

#a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
avg_sil <- silhouette(kmeans.result$cluster,dist(dataset)) 

#k-means clustering with estimating k and initializations
fviz_silhouette(avg_sil)
```
  
  
```{r}
# 1-  define function to compute average silhouette for k clusters using silhouette()
silhouette_score <- function(k){ 
km <- kmeans(dataset, centers = k,nstart=25) # if centers is a number, how many random sets should be chosen?
ss <- silhouette(km$cluster, dist(dataset))
sil<- mean(ss[, 3])
return(sil)
}
```


#__________________________________________k=5________________________________________________________________

#---------------------------k-means clustering 5---------------------------------------

#kmeansruns() : It calls  kmeans() to perform  k-means clustering
#It initializes the k-means algorithm several times with random points from the data set as means.
#It estimates the number of clusters by Calinski Harabasz index or average silhouette width يقلل عدد الكلستر 

#### K= 5 Clusters

```{r}
# run k-means clustering to find 4 clusters
#set a seed for random number generation  to make the results reproducible
set.seed(8953)
kmeans.result <- kmeans(dataset, 5)
# print the clusterng result
kmeans.result
```

#### visualize 4 clusters

```{r}
# visualize clustering (4 clusters)
#install.packages("factoextra")
library(factoextra)
fviz_cluster(kmeans.result, data = dataset)
```


#fviz_cluster(kmeans.result, data = dataset)


```{r}
plot(dataset[, c("age","class")], col = (kmeans.result$cluster) )
# this command add Points(center) to a Plot 
points(kmeans.result$centers[, c("age","class")],  col = 1:4, pch = 8, cex=10) 
```


```{r}
#performance k-means clustering
kmeans.result <- kmeans(dataset, 5)

# creat the initial plot
plot(dataset[, c("weight_kg","class")], col = (kmeans.result$cluster) )

# this command add Points(center) to a Plot (عشان حطينا 10)
points(kmeans.result$centers[, c("weight_kg","class")],  col = 1:4, pch = 8, cex=2)  
```


```{r}
#performance k-means clustering
kmeans.result <- kmeans(dataset, 5)

# creat the initial plot
plot(dataset[, c("body.fat_.","class")], col = (kmeans.result$cluster) )

# this command add Points(center) to a Plot (عشان حطينا 10)
points(kmeans.result$centers[, c("body.fat_.","class")],  col = 1:4, pch = 8, cex=2)  
```


#--------------------k-means clustering 2 ???????---------------------------
  
#kmeansruns() : It calls  kmeans() to perform  k-means clustering
#It initializes the k-means algorithm several times with random points from the data set as means.
#It estimates the number of clusters by Calinski Harabasz index or average silhouette width يقلل عدد الكلستر 

```{r}
#install.packages("fpc")
library(fpc)
kmeansruns.result <- kmeansruns(dataset)  
kmeansruns.result #4 mean cuz we have 4dimension
fviz_cluster(kmeansruns.result, data = dataset)
```


```{r}
#install.packages("cluster")
library(cluster)

# group into 4 clusters
pam.result <- pam(dataset, 5)

plot(pam.result)

#if some got -1 -> not on correct cluster
#if 0-> overlap 
```


```{r}
#average silhouette for each clusters (5 clusters)
library(cluster)
avg_sil <- silhouette(kmeans.result$cluster,dist(dataset)) #a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
fviz_silhouette(avg_sil)#k-means clustering with estimating k and initializations
```


#----------------------------Hierarchical Clustering---------------------------------

##----Hierarchical Clustering of the USArrest Data-----##


```{r}
set.seed(2835)

# draw a sample of 40 records from the body performance data, so that the clustering plot will not be over crowded
idx <- sample(1:dim(dataset)[1], 40)
dataset <- dataset[idx, ]
```


```{r}
## hiercrchical clustering
library(factoextra) 
hc.cut <- hcut(dataset, k = 5, hc_method = "complete") # Computes Hierarchical Clustering and Cut the Tree
```


```{r}
#ليه حط الطول؟
# Visualize dendrogram
fviz_dend(hc.cut,rect = TRUE)  #logical value specifying whether to add a rectangle around groups.
```


```{r}
# Visualize cluster
fviz_cluster(hc.cut, ellipse.type = "convex") # Character specifying frame type. Possible values are 'convex', 'confidence' etc
```


#average silhouette for each clusters 
  - choosing k using average silhouette
Silhouette analysis

Silhouette analysis allows you to calculate how similar each observations is with the cluster it is assigned relative to other clusters. This metric (silhouette width) ranges from -1 to 1 for each observation in your data and can be interpreted as follows:

    Values close to 1 suggest that the observation is well matched to the assigned cluster
    Values close to 0 suggest that the observation is borderline matched between two clusters
    Values close to -1 suggest that the observations may be assigned to the wrong cluster

In this exercise you will leverage the pam() and the silhouette() functions from the cluster library to perform silhouette analysis to compare the results of models with a k of 2 and a k of 3. You’ll continue working with the lineup dataset.

    Pay close attention to the silhouette plot, does each observation clearly belong to its assigned cluster for k 
  
تفسيرنا


```{r}
#average silhouette for each clusters 
library(cluster)

#a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
avg_sil <- silhouette(kmeans.result$cluster,dist(dataset)) 

#k-means clustering with estimating k and initializations
fviz_silhouette(avg_sil)
```
  
  


### Validation (Evaluation and Comparison) :

### Within cluster sums of squares:

#### 3- Elbow method

```{r}
#fviz_nbclust() with within cluster sums of squares (wss) method
library(factoextra) 
fviz_nbclust(dataset, kmeas, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```


```{r}
# 1-  define function to compute average silhouette for k clusters using silhouette()
silhouette_score <- function(k){ 
km <- kmeans(dataset, centers = k,nstart=25) # if centers is a number, how many random sets should be chosen?
ss <- silhouette(km$cluster, dist(dataset))
sil<- mean(ss[, 3])
return(sil)
}
```



```{r}
# wws

wssplot <- function(data, nc = 15, seed = 1234) {
  wss <- (nrow(data) - 1) * sum(apply(data, 2, var))
  for (i in 2:nc) {
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers = i)$withinss)
  }
  plot(1:nc, wss, type = "b", xlab = "Number of Clusters", ylab = "Within groups sum of squares")
  wss
}

# Call the function with your dataset 'brain_stroke'
result <- wssplot(brain_stroke)
```

### Average Silhouette

```{r}
#طريقة اليوتيوب
library(factoextra)
fviz_nbclust(brain_stroke, kmeans, method = "silhouette")
```


```{r}
#طريقة السلايدز حقت اللاب
# Define a function to compute average silhouette for k clusters using silhouette()
library(cluster)
silhouette_score <- function(k, data) {
  km <- kmeans(data, centers = k, nstart = 25)
  ss <- silhouette(km$cluster, dist(data))
  sil <- mean(ss[, 3])
  return(sil)
}

# Set the range of clusters (k values)
k_range <- 2:10

# Call the function for each k value and store the results in avg_sil
avg_sil <- sapply(k_range, silhouette_score, data = brain_stroke)

# Create a plot to visualize the Average Silhouette Scores
plot(k_range, avg_sil, type = 'b', xlab = 'Number of clusters', ylab = 'Average Silhouette Scores', frame = FALSE)
```


```{r}
#تكملة السلايدز حقت اللاب
# 2- silhouette method
install.packages("NbClust")
library(NbClust)
#a)fviz_nbclust() with silhouette method using library(factoextra) 
fviz_nbclust(USArrests, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
#b) NbClust validation
fres.nbclust <- NbClust(brain_stroke, distance="euclidean", min.nc = 2, max.nc = 10, method="kmeans", index="all")
```


```{r}
##  k cluster range from 2 to 10
k <- 2:10
##  call  function fore k value
avg_sil <- sapply(k, silhouette_score)  ##Apply a Function over a List or Vector
plot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)

```

#### 2- silhouette method

```{r}
#install.packages("NbClust")
library(NbClust)
#a)fviz_nbclust() with silhouette method using library(factoextra) 
fviz_nbclust(dataset, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
#b) NbClust validation
fres.nbclust <- NbClust(dataset, distance="euclidean", min.nc = 2, max.nc = 10, method="kmeans", index="all")
```


#====================== Garbage:
```{r}
#install.packages("fpc")
library(fpc)
```


```{r}
set.seed(8953)

kmeans.result2 <- kmeans(dataset, 2)
```



```{r}
kmeansruns.result2 <- kmeansruns(dataset)  
kmeansruns.result2
```


```{r}
fviz_cluster(kmeansruns.result2, data = dataset)
```

#--------------------------------k-mediods clustering with PAM------------------------------


```{r}
#avr????????????
library(cluster) 
#sil <- silhouette(km$cluster, dist(dataset))
#rownames(sil) <- rownames(dataset)
#fviz_silhouette(sil)
```


#-----------------------Hierarchical Clustering-------------------------------

##----Hierarchical Clustering of the USArrest Data-----##


```{r}
set.seed(2835)

# draw a sample of 40 records from the body performance data, so that the clustering plot will not be over crowded
idx <- sample(1:dim(dataset)[1], 40)
daraset <- dataset[idx, ]

## hiercrchical clustering
library(factoextra) 
hc.cut <- hcut(dataset, k = 2, hc_method = "complete") # Computes Hierarchical Clustering and Cut the Tree

```


```{r}
# Visualize dendrogram
fviz_dend(hc.cut,rect = TRUE)  #logical value specifying whether to add a rectangle around groups.
```


```{r}
# Visualize cluster
fviz_cluster(hc.cut, ellipse.type = "convex") 
# Character specifying frame type. Possible values are 'convex', 'confidence' etc
```

#==========end garbage

----------------------------------------------


```{r}

library(factoextra)
fviz_nbclust(data, kmeans, method="wss")
fviz_nbclust(data, kmeans, method="silhouette")
fviz_nbclust(data, kmeans, method="gap_start")
```



# visualize clusters using original variables
clusters <- kmeans(data, centers = 3, iter.max = 100, nstart = 100)
Stroke <- data |> mutate(cluster = clusters$cluster)
stroke |> ggplot(aes(x = Rating, y = Price, col = as.factor(cluster))) + geom_point()

     

```{r}
fviz_nbclust(USArrests, kmeans, method = "wss") +
geom_vline(xintercept= 4, linetype= 2)+
labs(subtitle = "Elbow method")
```



```{r}
fviz_nbclust(USArrests, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
```

